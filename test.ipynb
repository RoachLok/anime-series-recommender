{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession, functions\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, FloatType\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('Anime_recommender').getOrCreate()\n",
    "\n",
    "ratings     = spark.read.csv('data/rating_complete_small.csv', sep=',', header=True, \n",
    "                    schema= StructType([ \n",
    "                            StructField('user'   , IntegerType()), \n",
    "                            StructField('item'   , IntegerType()),\n",
    "                            StructField('rating' , IntegerType()) \n",
    "                        ]))\n",
    "\n",
    "EP_ratings  = spark.read.csv('data/valoraciones_EP.csv', sep=',', header=True, \n",
    "                    schema= StructType([ \n",
    "                            StructField('user'   , IntegerType()), \n",
    "                            StructField('item'   , IntegerType()),\n",
    "                            StructField('rating' , IntegerType()) \n",
    "                        ]))\n",
    "series      = spark.read.csv('data/anime.csv', sep=',', header=True,\n",
    "                    schema=StructType([ \n",
    "                            StructField('item'     , IntegerType()), \n",
    "                            StructField('Name'   , StringType()),\n",
    "                            StructField('Score'  , FloatType()) ,\n",
    "                            StructField('Genres'  , StringType()) ,\n",
    "                            StructField('English name'  , StringType()) ,\n",
    "                            StructField('Japanese name'  , StringType()) ,\n",
    "                            StructField('Type'  , StringType()) \n",
    "                        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in data: 98.87\n"
     ]
    }
   ],
   "source": [
    "all_ratings = ratings.union(EP_ratings)\n",
    "\n",
    "all_ratings_wtype = all_ratings.join(series['item', 'Type'], on='item')\n",
    "\n",
    "n_ratings = all_ratings_wtype.select(\"rating\").count()\n",
    "\n",
    "n_users = all_ratings_wtype.select(\"user\").distinct().count()\n",
    "n_items = all_ratings_wtype.select(\"item\").distinct().count()\n",
    "\n",
    "sparse_percentage = (1.0 - (n_ratings *1.0)/(n_users * n_items))*100\n",
    "\n",
    "print(\"Sparsity in data:\", round(sparse_percentage, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| user|count|\n",
      "+-----+-----+\n",
      "|68042|13462|\n",
      "|10255| 8215|\n",
      "|64807| 6656|\n",
      "|38143| 6043|\n",
      "| 4773| 5616|\n",
      "|18355| 5311|\n",
      "|71931| 5088|\n",
      "|25411| 4862|\n",
      "|55748| 4810|\n",
      "|15609| 4593|\n",
      "|23930| 4579|\n",
      "|34764| 4478|\n",
      "|63339| 4266|\n",
      "| 7179| 4045|\n",
      "|32075| 3982|\n",
      "|67428| 3975|\n",
      "|54171| 3741|\n",
      "|71433| 3685|\n",
      "|10367| 3603|\n",
      "| 5648| 3571|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_ratings_user = ratings.groupBy(\"user\").count().orderBy('count', ascending=False)\n",
    "n_ratings_user.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| item|count|\n",
      "+-----+-----+\n",
      "| 1959|   40|\n",
      "| 4190|   40|\n",
      "| 5217|   40|\n",
      "| 2610|   40|\n",
      "| 5383|   40|\n",
      "| 4025|   40|\n",
      "| 7504|   40|\n",
      "| 5953|   40|\n",
      "|15159|   40|\n",
      "|10869|   40|\n",
      "|38036|   40|\n",
      "|34503|   40|\n",
      "| 3890|   40|\n",
      "|13029|   40|\n",
      "|30086|   40|\n",
      "| 3529|   40|\n",
      "|43465|   40|\n",
      "|28641|   40|\n",
      "| 3726|   40|\n",
      "| 2803|   40|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_ratings_movie = ratings.groupBy(\"item\").count().where('count > 39').orderBy('count', ascending=True)\n",
    "n_ratings_movie.show() # 16463 -> 9144\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num models to be tested:  4\n"
     ]
    }
   ],
   "source": [
    "(train, test) = ratings.randomSplit([0.8, 0.2], seed = 1234)\n",
    "\n",
    "als = ALS(userCol=\"user\", itemCol=\"item\", ratingCol=\"rating\", nonnegative = True, implicitPrefs = False, coldStartStrategy=\"drop\")\n",
    "\n",
    "# Add hyperparameters and their respective values to param_grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 150]) \\\n",
    "            .addGrid(als.regParam, [.01,.15]) \\\n",
    "            .build()\n",
    "            #             .addGrid(als.maxIter, [5, 50, 100, 200]) \\\n",
    "\n",
    "           \n",
    "# Define evaluator as RMSE and print length of evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n",
    "print (\"Num models to be tested: \", len(param_grid))\n",
    "\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(train)\n",
    "\n",
    "best_model = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_ratings = ratings.union(EP_ratings)\n",
    "# DONE ABOVE\n",
    "#all_ratings_wtype = all_ratings.join(series['item', 'Type'], on='item')\n",
    "\n",
    "model = als.fit(all_ratings)\n",
    "model.setPredictionCol('predic')\n",
    "#model.userFactors.orderBy(\"id\").collect()\n",
    "\n",
    "type(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# View the predictions\n",
    "test_predictions = best_model.transform(test)\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE)\n",
    "test_predictions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALS_174cfffc4c1a"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als = ALS(rank=10, seed=0)\n",
    "\n",
    "als.setMaxIter(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EP_ratings.show(4)\n",
    "#ratings.show(4)\n",
    "#\n",
    "#all_ratings = ratings.union(EP_ratings)\n",
    "#\n",
    "#all_ratings.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_ratings = ratings.union(EP_ratings)\n",
    "# DONE ABOVE\n",
    "#all_ratings_wtype = all_ratings.join(series['item', 'Type'], on='item')\n",
    "\n",
    "model = als.fit(all_ratings)\n",
    "model.setPredictionCol('predic')\n",
    "#model.userFactors.orderBy(\"id\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import when\n",
    "not_seen = all_ratings_wtype.withColumn('user', when(all_ratings_wtype['user'] != 666666, 666666).otherwise(-1)).where('Type == \"TV\" or Type == \"Movie\"').dropDuplicates(['item']).where('user != -1')\n",
    "\n",
    "#predictions = sorted(model.transform(not_seen['user','item','Type']).where('Type == \"TV\"').collect(), key=lambda r: r[3], reverse=True)\n",
    "\n",
    "#model.transform(not_seen['user','item','Type']).sort('Type', 'predic', ascending=False).show(300)\n",
    "\n",
    "test = model.transform(not_seen['user','item', 'Type']).sort('Type', 'predic', ascending=False).collect()\n",
    "\n",
    "print(type(test))\n",
    "if test[len(test)//2]['Type'] == 'Movie':\n",
    "    pass\n",
    "elif test[3*len(test)//5]['Type'] == 'Movie':\n",
    "    for i in range (2*len(test)//3, 0, -1):\n",
    "        print(test[i]['Type'])\n",
    "        if test[i]['Type'] == \"TV\":\n",
    "            print(i)\n",
    "            break\n",
    "elif test[4*len(test)//5]['Type'] == 'Movie':\n",
    "    for i in range (3*len(test)//4, 0, -1):\n",
    "        print(test[i]['Type'])\n",
    "        if test[i]['Type'] == \"TV\":\n",
    "            print(i,2)\n",
    "            break\n",
    "#model.transform(not_seen['user','item','Type']).sort('Type', 'predic', ascending=False).show(300)\n",
    "#print(predictions[:10])\n",
    "\n",
    "#predictions = model.transform(not_seen['user','item','Type']).sort(['Type','predic'], ascending=False).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|user| item|count|\n",
      "+----+-----+-----+\n",
      "|   0| 2248|    1|\n",
      "|   3|36038|    1|\n",
      "|   3|14199|    1|\n",
      "+----+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+----+------+\n",
      "|  user|item|rating|\n",
      "+------+----+------+\n",
      "|666666| 430|     9|\n",
      "|666666|1004|     5|\n",
      "|666666|3010|     7|\n",
      "+------+----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "all_ratings = ratings.union(EP_ratings)\n",
    "test = all_ratings.groupBy('user','item').count().where('user != 666666')\n",
    "\n",
    "other = all_ratings.withColumn('user', when(all_ratings['user'] != 666666, 666666))\n",
    "\n",
    "test.show(3)\n",
    "other.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|user| item|count|\n",
      "+----+-----+-----+\n",
      "|   0| 2248|    1|\n",
      "|   3|36038|    1|\n",
      "|   3|14199|    1|\n",
      "|   6| 1496|    1|\n",
      "|   7|22199|    1|\n",
      "|  14|38524|    1|\n",
      "|  16| 2508|    1|\n",
      "|  17|24459|    1|\n",
      "|  17| 6919|    1|\n",
      "|  17|10719|    1|\n",
      "+----+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = all_ratings.groupBy('user','item').count().where('user != 666666')\n",
    "test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+\n",
      "|  user| item|rating|\n",
      "+------+-----+------+\n",
      "|666666|10396|     8|\n",
      "|666666| 4353|     7|\n",
      "|666666| 1335|     8|\n",
      "|666666|  143|    10|\n",
      "|666666| 3588|     8|\n",
      "|666666|  392|     6|\n",
      "|666666|32438|     5|\n",
      "|666666| 2251|     8|\n",
      "|666666| 6702|     8|\n",
      "|666666|28735|     8|\n",
      "+------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "other = all_ratings.withColumn('user', when(all_ratings['user'] != 666666, 666666)).dropDuplicates()\n",
    "other.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtabo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "user_recs = model.recommendForAllUsers(50)\n",
    "#test = user_recs.where(user_recs.user == 666666).select(\"recommendations.item\", \"recommendations.rating\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  user|     recommendations|\n",
      "+------+--------------------+\n",
      "|666666|[{35054, 12.45934...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = user_recs.where(user_recs.user == 666666).show(10)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|user|item|rating|\n",
      "+----+----+------+\n",
      "|   0| 430|     9|\n",
      "|   0|1004|     5|\n",
      "|   0|3010|     7|\n",
      "+----+----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#series['id', 'Type'][series['id'].isin(35035)].where('Type == \"Movie\"')\n",
    "\n",
    "all_ratings.join(series['id', 'Type'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|   id|Type|\n",
      "+-----+----+\n",
      "|18743| OVA|\n",
      "|23731| OVA|\n",
      "|26091| OVA|\n",
      "|26281| OVA|\n",
      "|26293| OVA|\n",
      "|28561| OVA|\n",
      "|29365| OVA|\n",
      "|30941| OVA|\n",
      "|31004| OVA|\n",
      "|33823| OVA|\n",
      "+-----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#series['id', 'Type'].filter(series.id.isin()).show(2)\n",
    "series['id', 'Type'][series['id'].isin(test[0]['item'])].where('Type == \"OVA\"').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+----+----+\n",
      "|user|item|rating|  id|Type|\n",
      "+----+----+------+----+----+\n",
      "|   0|3010|     7|3010|  TV|\n",
      "|   0|2762|     9|2762|  TV|\n",
      "|   0|1571|    10|1571|  TV|\n",
      "+----+----+------+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "series = spark.read.csv('data/anime.csv', header=True)\n",
    "\n",
    "all_ratings.join(series['id', 'Type'], all_ratings['item'] == series['id']).where('Type == \"TV\"').show(3)\n",
    "\n",
    "# series['id', 'Type'].show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://rr4---sn-gqn-h5qs.googlevideo.com/videoplayback?expire=1640169994&ei=qq3CYd2hBpCEWIzPkOAD&ip=2a0c%3A5a80%3A4703%3Ab600%3A512f%3A8d8b%3Ac19%3A797b&id=o-AHtGT5vPyCuaNB6ckEJv8GQiH1WNPpyT-xcGikkW8Kkm&itag=248&source=youtube&requiressl=yes&mh=6e&mm=31%2C29&mn=sn-gqn-h5qs%2Csn-h5q7knel&ms=au%2Crdu&mv=m&mvi=4&pcm2cms=yes&pl=36&initcwndbps=1175000&vprv=1&mime=video%2Fwebm&gir=yes&clen=9868364&dur=60.059&lmt=1633383944636798&mt=1640148061&fvip=4&keepalive=yes&fexp=24001373%2C24007246&c=ANDROID&txp=5316224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cvprv%2Cmime%2Cgir%2Cclen%2Cdur%2Clmt&sig=AOq0QJ8wRgIhANzNPKjtpI6lKfF7EEMrhfbfa7iga9EUn3kypoFmZQjFAiEA29Rhmug67_LKyLunBtbz3O_sAFRSvHhREDO4tAVXbZY%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpcm2cms%2Cpl%2Cinitcwndbps&lsig=AG3C_xAwRgIhAJ6A_o1rCUR-r9TH483yB-qSZGJfKBZWfXpWrBIW1aC5AiEAyEAH5-Uu3h1KfimzlK9vhRc9AjdGQZWw0-8PdpV21uw%3D',\n",
       " 'https://rr4---sn-gqn-h5qs.googlevideo.com/videoplayback?expire=1640169994&ei=qq3CYd2hBpCEWIzPkOAD&ip=2a0c%3A5a80%3A4703%3Ab600%3A512f%3A8d8b%3Ac19%3A797b&id=o-AHtGT5vPyCuaNB6ckEJv8GQiH1WNPpyT-xcGikkW8Kkm&itag=251&source=youtube&requiressl=yes&mh=6e&mm=31%2C29&mn=sn-gqn-h5qs%2Csn-h5q7knel&ms=au%2Crdu&mv=m&mvi=4&pcm2cms=yes&pl=36&initcwndbps=1175000&vprv=1&mime=audio%2Fwebm&gir=yes&clen=846940&dur=60.141&lmt=1633383944465944&mt=1640148061&fvip=4&keepalive=yes&fexp=24001373%2C24007246&c=ANDROID&txp=5311224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cvprv%2Cmime%2Cgir%2Cclen%2Cdur%2Clmt&sig=AOq0QJ8wRQIhAPMmxiI-9Ff1YZc6o69ewNu3eW3LAnBbily6Raoe3wkjAiADUMm4Z3dFDU3P-LDXvIGUCdepcmxnnSQcpGW4Vma58Q%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpcm2cms%2Cpl%2Cinitcwndbps&lsig=AG3C_xAwRgIhAJ6A_o1rCUR-r9TH483yB-qSZGJfKBZWfXpWrBIW1aC5AiEAyEAH5-Uu3h1KfimzlK9vhRc9AjdGQZWw0-8PdpV21uw%3D\\n']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "dlp_out = subprocess.run(['yt-dlp', '--get-url', 'ytsearch1:\"cowboy bebop trailer anime series original\"'], stdout=subprocess.PIPE)\n",
    "dlp_out.stdout.decode('utf-8').split('\\n',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oqd2C3oZkBU\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlp_out = subprocess.run(['yt-dlp', '--get-id', 'ytsearch1:\"cowboy bebop trailer anime series\"'], stdout=subprocess.PIPE)\n",
    "dlp_out.stdout.decode('utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a3f705af5d16188ac4dab049ec5b9b47d5753cd6a4b7152e30b120490c12182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
